<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Neural RGB-D Surface Reconstruction - Surface reconstruction from color and depth images using a multi-layer perceptron.">
  <meta name="keywords" content="neural, rgbd, rgb-d, surface, geometry, reconstruction">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Neural RGB-D Surface Reconstruction</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/juxtapose.css">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="http://niessnerlab.org">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="http://niessnerlab.org/projects/azinovic2019inverse.html">
            Inverse Path Tracing
          </a>
          <a class="navbar-item" href="https://nihalsid.github.io/retrieval-fuse/">
            RetrievalFuse
          </a>
          <a class="navbar-item" href="https://pablopalafox.github.io/npms/">
            Neural Parametric Models
          </a>
          <a class="navbar-item" href="https://andreiburov.github.io/DSFN/">
            Dynamic Surface Function Networks
          </a>
          <a class="navbar-item" href="https://gafniguy.github.io/4D-Facial-Avatars/">
            4D Facial Avatars
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Neural RGB-D Surface Reconstruction</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="http://niessnerlab.org/members/dejan_azinovic/profile.html">Dejan Azinović</a><sup>1</sup>
            </span>
            <span class="author-block">
              <a href="http://www.ricardomartinbrualla.com">Ricardo Martin-Brualla</a><sup>2</sup>
            </span>
            <span class="author-block">
              <a href="https://www.danbgoldman.com">Dan B Goldman</a><sup>2</sup>
            </span>
            <span class="author-block">
              <a href="http://niessnerlab.org/members/matthias_niessner/profile.html">Matthias Nießner</a><sup>1</sup>
            </span>
            <span class="author-block">
              <a href="https://justusthies.github.io/">Justus Thies</a><sup>1, 3</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Technical University of Munich</span>
            <span class="author-block"><sup>2</sup>Google Research</span>
            <span class="author-block"><sup>3</sup>Max Planck Institute for Intelligent Systems</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="static/pdf/neural_rgbd_surface_reconstruction.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2104.04532"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=iWuSowPsC3g"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/dazinovic/neural-rgbd-surface-reconstruction"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
    <div class="columns is-centered">
      <div class="column is-6">
        <video id="teaser" autoplay muted loop height="100%">
          <source src="static/videos/teaser.mp4"
                  type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          Our method obtains a high-quality 3D reconstruction from an RGB-D input sequence by training a multi-layer perceptron.
        </h2>
      </div>
    </div>
</section>

<section class="section">
  <div class="container">

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-two-thirds">
        <h2 class="title is-2">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            In this work, we explore how to leverage the success of implicit novel view synthesis methods for surface reconstruction.
            Methods which learn a neural radiance field have shown amazing image synthesis results, but the underlying geometry representation is only a coarse approximation of the real geometry.
            We demonstrate how depth measurements can be incorporated into the radiance field formulation to produce more detailed and complete reconstruction results than using methods based on either color or depth data alone.
            In contrast to a density field as the underlying geometry representation, we propose to learn a deep neural network which stores a truncated signed distance field.
            Using this representation, we show that one can still leverage differentiable volume rendering to estimate color values of the observed images during training to compute a reconstruction loss.
            This is beneficial for learning the signed distance field in regions with missing depth measurements.
            Furthermore, we correct for misalignment errors of the camera, improving the overall reconstruction quality.
            In several experiments, we show-cast our method and compare to existing works on classical RGB-D fusion and learned representations.
          </p>
        </div>
      </div>
    </div>
  </div>
  <!--/ Abstract. -->
</section>

<section>
  <!-- Paper video. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-6">
      <h2 class="title is-2">Video</h2>
      <div class="publication-video">
        <iframe src="https://www.youtube.com/embed/iWuSowPsC3g?rel=0&amp;showinfo=0"
                frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      </div>
    </div>
  </div>
  <!--/ Paper video. -->
</section>

<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h3 class="title is-2">Results</h3>
        <div class="content has-text-justified">
          <p>
            We test our method on the <a href="http://www.scan-net.org/">ScanNet</a> dataset which provides RGB-D sequences of room-scale scenes.
            We compare our method to the original ScanNet <a href="http://graphics.stanford.edu/projects/bundlefusion/">BundleFusion</a> reconstructions which often suffer from severe camera pose misalignment.
            Our approach jointly optimizes for the scene representation network as well as the camera poses, leading to substantially reduced misalignment artifacts in the reconstructed geometry.
          </p>
        </div>
        <div class="columns is-centered">
          <div class="column is-three-quarters">
            <div class="content juxtapose">
              <img src="static/images/scene0002_bf.png" data-label="BundleFusion [Dai et al. 2017]" />
              <img src="static/images/scene0002_ours.png" data-label="Ours" />
            </div>
            <div class="content juxtapose">
              <img src="static/images/scene0050_bf.png" data-label="BundleFusion [Dai et al. 2017]" />
              <img src="static/images/scene0050_ours.png" data-label="Ours" />
            </div>
            <script src="./static/js/juxtapose.min.js"></script>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{azinovic2022neural,
    title     = {Neural RGB-D Surface Reconstruction}, 
    author    = {Dejan Azinovi{\'c} and Ricardo Martin-Brualla and Dan B Goldman and Matthias Nie{\ss}ner and Justus Thies},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="static/pdf/neural_rgbd_surface_reconstruction.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="#" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website, we just ask that you link back to the <a href="https://nerfies.github.io/">original page</a> in the footer.
            Please remember to remove the analytics code included in the header of the website which you do not want on your website.
          </p>
          <p>
            The comparison slider was implemented with <a href="https://juxtapose.knightlab.com/">Juxtapose</a>, which is licensed under <a href="https://raw.githubusercontent.com/NUKnightLab/juxtapose/master/LICENSE">Mozilla Public License Version 2.0</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>